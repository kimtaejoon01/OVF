{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f74708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nas4_user/sibeenkim/anaconda3/envs/ovf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "\n",
    "from preprocess import read_dicom_series, n4_bias_correction, get_bbox, crop\n",
    "from model import OVFNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059430f",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d843371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = [\"083\", \"084\"]\n",
    "\n",
    "dicom_path = [\n",
    "    \"./sample_dicom/083_T1\",\n",
    "    \"./sample_dicom/084_T1\"\n",
    "]\n",
    "\n",
    "# Keypoint data need to be pre-defined.\n",
    "keypoint_data = [\n",
    "    {'keyframe': 6, 'keypoints': [[397.64, 189.88], [368.43, 176.72], [385.3, 243.77], [348.27, 238.84], [363.49, 209.63], [389.41, 219.5]]},\n",
    "    {'keyframe': 7, 'keypoints': [[143.39, 198.16], [145.86, 252.46], [180.82, 247.11], [169.31, 196.1], [155.32, 225.31], [170.13, 227.78]]}\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"Age\": 57, \"Gender\": 1, \"BMD\": -3.4, \"PreFractureDrugIntake\": 0, \"PostFractureDrugIntake\": 1},\n",
    "    {\"Age\": 69, \"Gender\": 0, \"BMD\": -4.3, \"PreFractureDrugIntake\": 0, \"PostFractureDrugIntake\": 1}\n",
    "]\n",
    "\n",
    "Y = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3312cbf",
   "metadata": {},
   "source": [
    "# Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10017c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ensemble = 1 # Set to 1 if probability ensemble is desired. Else 0. Just leave it as 1 unless you have less than 3 consecutive DICOM frames (which is unlikely) prepared for a sample.\n",
    "use_clinical = 0 # Set to 1 if using metadata is desired. Else 0. Unlike ensemble, using clinical is not strictly better in performance. Your choice. I've prepared checkpoints for both 0 and 1.\n",
    "ckpt_path = f\"./best_checkpoint/use_clinical_{use_clinical}.pth\"\n",
    "optimal_threshold = [0.5644413232803345, 0.6101016998291016][use_clinical] # Determined in training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5b694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OVFNet(\n",
       "  (image_encoder): _LoRA_ViT_timm(\n",
       "    (lora_vit): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv_timm(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_q): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_k): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_k): Linear(in_features=16, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=16, bias=False)\n",
       "              (linear_b_v): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = OVFNet(use_clinical=use_clinical)\n",
    "device = torch.device('cuda')\n",
    "network = network.to(device)\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "network.load_state_dict(checkpoint)\n",
    "\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66f480",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fc9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(224, 224), interpolation=v2.InterpolationMode.BICUBIC),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean=(0.48145466, 0.4578275, 0.40821073), # BiomedCLIP statistics: https://arxiv.org/abs/2303.00915\n",
    "        std=(0.26862954, 0.26130258, 0.27577711))  # BiomedCLIP statistics: https://arxiv.org/abs/2303.00915\n",
    "])\n",
    "\n",
    "X = []\n",
    "\n",
    "for i, _ in enumerate(patient_id):\n",
    "    dp = dicom_path[i]\n",
    "    img = read_dicom_series(dp)\n",
    "    img = n4_bias_correction(img)\n",
    "\n",
    "    keyframe = keypoint_data[i][\"keyframe\"]\n",
    "    keypoints = keypoint_data[i][\"keypoints\"]\n",
    "    keypoints = np.array(keypoints)\n",
    "    bbox = get_bbox(keypoints, expand_ratio=1.5)\n",
    "\n",
    "    slices = []\n",
    "\n",
    "    for j in [keyframe-1, keyframe, keyframe+1]: # This is for prob ensemble below.\n",
    "        slice = img[j, :, :]\n",
    "        slice = crop(slice, bbox)\n",
    "        slice = np.clip(slice, np.quantile(slice, 0.05), np.quantile(slice, 0.95))\n",
    "        slice = (slice - slice.min()) / (slice.max() - slice.min())\n",
    "        slice = (slice * 255).astype(np.uint8)\n",
    "        slice = np.tile(slice[..., None], (1, 1, 3)) # The '3' here is for duplicating channels from grayscale (1) to color (3), NOT for ensemble's 3 frames. Don't be confused.\n",
    "        slice = transforms(slice)\n",
    "        slices.append(slice)\n",
    "\n",
    "    x = torch.stack(slices)\n",
    "    X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76efc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = 72.77551020408163 # Pre-calculated value\n",
    "age_std = 9.378789520714859 # Pre-calculated value\n",
    "bmd_mean = -3.154285714285714 # Pre-calculated value\n",
    "bmd_std = 1.0773905938902932 # Pre-calculated value\n",
    "\n",
    "for md in metadata:\n",
    "    md['Age'] = (md['Age'] - age_mean) / age_std\n",
    "    md['BMD'] = (md['BMD'] - bmd_mean) / bmd_std\n",
    "\n",
    "columns = ['Age', 'Gender', 'BMD', 'PreFractureDrugIntake', 'PostFractureDrugIntake']\n",
    "M = [torch.tensor([md[c] for c in columns]) for md in metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5944644",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bef7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    for x, m, y in zip(X, M, Y):\n",
    "        m = m.unsqueeze(0)\n",
    "        x, m, y = x.to(device), m.to(device), y.to(device)\n",
    "\n",
    "        if use_ensemble:\n",
    "            positive_probs = []\n",
    "\n",
    "            for i in range(3):\n",
    "                logit = network(x[i, None], m).squeeze()\n",
    "                positive_prob = torch.sigmoid(logit)\n",
    "                positive_probs.append(positive_prob)\n",
    "                \n",
    "            positive_prob = torch.stack(positive_probs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            logit = network(x[1, None], m).squeeze()\n",
    "            positive_prob = torch.sigmoid(logit)\n",
    "            \n",
    "        outputs.append({'positive_prob': positive_prob, 'y_true': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34bef80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive_prob': tensor(0.1072, device='cuda:0'), 'y_true': tensor(0, device='cuda:0')}\n",
      "{'positive_prob': tensor(0.7573, device='cuda:0'), 'y_true': tensor(1, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f7374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = []\n",
    "pr_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    positive_probs = torch.cat([output['positive_prob'].unsqueeze(0) for output in outputs]).cpu().numpy().squeeze()\n",
    "    all_y_trues = torch.cat([output['y_true'].unsqueeze(0) for output in outputs]).cpu().numpy().squeeze()\n",
    "\n",
    "    auc = roc_auc_score(all_y_trues, positive_probs)\n",
    "    fpr, tpr, thresholds = roc_curve(all_y_trues, positive_probs)\n",
    "    # ix = np.argmax(tpr - fpr)\n",
    "    # optimal_threshold = thresholds[ix]\n",
    "\n",
    "    preds_binary = (positive_probs  > optimal_threshold).astype(int)\n",
    "    f1 = f1_score(all_y_trues, preds_binary)\n",
    "    accuracy = accuracy_score(all_y_trues, preds_binary)\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_y_trues, preds_binary)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    metrics = { 'auc': auc, 'accuracy': accuracy, 'specificity': specificity, 'sensitivity': sensitivity, 'f1': f1 }\n",
    "    pr = { 'tpr': tpr, 'fpr': fpr }\n",
    "\n",
    "    metrics_all.append(metrics)\n",
    "    pr_all.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fdd5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': np.float64(1.0), 'accuracy': 1.0, 'specificity': np.float64(1.0), 'sensitivity': np.float64(1.0), 'f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics_all:\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d374261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tpr': array([0., 1., 1.]), 'fpr': array([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "for pr in pr_all:\n",
    "    print(pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
